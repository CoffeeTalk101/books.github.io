<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>MapReduce任务两种OOM问题的排查处理</title>
  <meta name="keywords" content="BigData"/>
  <link href="../Styles/Style0001.css" type="text/css" rel="stylesheet"/>
</head>

<body>
<h1>MapReduce任务两种OOM问题的排查处理</h1>

<p class="date">2020-10-15</p>

<a class="home" href="https://xdcsy.github.io/">&lt; view all posts</a>

<p>在特征工程的MR任务中，目前遇到两种不同的OOM报错，处理方法也不同。</p>

<p><b>第一种OOM，报错信息如下</b>：</p>

<!-- HTML generated using hilite.me --><div style="background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#10
at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: java.lang.OutOfMemoryError: Java heap space
</pre>
</div>

<p>报错信息显示在reduce的shuffle阶段，Java的堆空间不足，那么能想到比较直接的方法是增加分配给每个reducer的内存空间，但是实际上这样不起作用。</p>

<p>通过进一步查找官方论坛的<a href="https://issues.apache.org/jira/browse/MAPREDUCE-6447">issue</a>，发现这是Hadoop的一个bug导致的。</p>

<p>这个bug的起因是Hadoop对两个参数的默认值配置不正确，分别是：<b>mapreduce.reduce.shuffle.input.buffer.percent</b> 以及 <b>mapreduce.reduce.shuffle.memory.limit.percent</b>。前一个参数规定了shuffle阶段的总数据能够占用内存的多大比例，而后一个参数规定了每个shuffle任务能够使用这些内存的比例，超出这个空间的数据就不保存在内存，而是会暂存在硬盘上。</p>

<p>这两个参数在Hadoop中的默认值是0.9和0.25，而并行的Fetcher线程数默认是5个，那么我们可以计算出，<b>极限情况下shuffle会被分配到 0.9*0.25*5=1.125 比例的内存，已经超出了100%</b>，所以会导致内存溢出。</p>

<p>定位到问题之后，解决的方法就是降低分配给 shuffle 的内存的比例，让计算过程中数据更多地存到硬盘而非内存上。目前使用如下的值可以避免这个问题：</p>

<p>mapreduce.reduce.shuffle.input.buffer.percent = 0.4</p>

<p>mapreduce.reduce.shuffle.memory.limit.percent = 0.15</p>

<p><b>第二种OOM，报错信息如下</b>：</p>

<!-- HTML generated using hilite.me --><div style="background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">INFO [communication thread] org.apache.hadoop.mapred.Task: Communication exception: java.lang.OutOfMemoryError: Java heap space
at sun.nio.cs.UTF_8.newDecoder(UTF_8.java:68)
at java.lang.StringCoding$StringDecoder.<span style="color: #330099; font-weight: bold">&lt;init&gt;</span>(StringCoding.java:131)
at java.lang.StringCoding$StringDecoder.<span style="color: #330099; font-weight: bold">&lt;init&gt;</span>(StringCoding.java:122)
at java.lang.StringCoding.decode(StringCoding.java:187)
at java.lang.String.<span style="color: #330099; font-weight: bold">&lt;init&gt;</span>(String.java:416)
at java.lang.String.<span style="color: #330099; font-weight: bold">&lt;init&gt;</span>(String.java:481)
at java.io.UnixFileSystem.list(Native Method)
at java.io.File.list(File.java:1116)
at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.getProcessList(ProcfsBasedProcessTree.java:459)
at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.updateProcessTree(ProcfsBasedProcessTree.java:201)
at org.apache.hadoop.mapred.Task.updateResourceCounters(Task.java:845)
at org.apache.hadoop.mapred.Task.updateCounters(Task.java:984)
at org.apache.hadoop.mapred.Task.access$500(Task.java:78)
at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:733)
at java.lang.Thread.run(Thread.java:745)
</pre>
</div>

<p>也是在reduce阶段出现了Java堆空间不足的情况，但是具体原因和上面不同。</p>

<p>通过报错信息可以看到，是yarn的内存调度时，创建用来保存线程树的字符串时出现问题。</p>

<p>进一步分析具体原因，虽然我们已经通过配置 <b>mapreduce.reduce.memory.mb</b> ，给每个reduce分配了比较大的内存，但是这些内存大部分是用来缓存reduce使用和生成的数据，而启动reducer所在的JVM虚拟机时使用的内存则是由另一个参数，<b>mapreduce.reduce.java.opts</b> 来定义的。这个参数的默认值并不大（200m），因此会造成JVM的资源不足。</p>

<p>将 mapreduce.reduce.java.opts 配置为2048m即可解决问题。需要注意的是 mapreduce.reduce.java.opts（给reducer的JVM的内存） 必须要小于 mapreduce.reduce.memory.mb（一个reducer能用的总内存）。</p>

<hr/>
<p class="footer">This blog is written in  EPUB.
The original file is available for <a href="https://github.com/XDcsy/XDcsy.github.io/raw/main/blog.epub">download</a>.</p>
<p class="footer">Feel free to comment by <a href="https://github.com/XDcsy/XDcsy.github.io/issues/new">raising an issue <img alt="GitHub-Mark-32px" src="../Images/GitHub-Mark-32px.png" class="github"/></a>.</p>
</body>
</html>