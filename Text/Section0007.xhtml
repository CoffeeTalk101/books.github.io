<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>XGBoost 简介（2）</title>
  <meta name="keywords" content="AI" />
<link href="../Styles/Style0001.css" type="text/css" rel="stylesheet"/>
</head>

<body>
<h1>XGBoost 简介（2）</h1>

<p class="date">2020-09-26</p>

<a class="home" href="https://xdcsy.github.io/">&lt; view all posts</a>

<p>XGBoost是于2015年由陈天奇等人提出的一个新的Gradient Boosting的实现。名称中的X代表 Extreme，意思是XGBoost旨在将Gradient Boosting对资源的利用和计算能力推至极限。</p>
<blockquote>
    <p>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.</p>

<p>—— 陈天奇</p>
</blockquote>

<h2 class="sigil_not_in_toc">XGBoost在原理上的优化</h2>

<p>尽管XGBoost最大的优势在于其对计算速度和资源使用的优化，但对于Gradient Boosting算法本身，XGBoost同样进行了改进。主要体现在对于模型优化过程的改进：</p>

<p>传统的Gradient Boosting在作梯度下降时只使用一阶导数信息，而XGBoost中对目标函数进行二阶泰勒展开，引入二阶导数信息，推导出一种新的对决策树划分增益的计算方法，新的方法在优化目标函数上更加有效和精确。</p>

<p>具体的推导过程可以参考<a href="https://www.cnblogs.com/massquantity/p/9794480.html">这篇文章</a>。</p>

<h2 class="sigil_not_in_toc">XGBoost在实现上的优化</h2>

<ul>
    <li>在添加新树时对于行、列的随机采样；在划分树时对列的随机采样.</li>
    <li>目标函数添加L1和L2正则项防止过拟合.</li>
    <li>创建决策树时可以利用多核CPU进行并行计算.</li>
    <li>支持在计算机集群上作分布式计算.</li>
    <li>对数据量过大无法存入内存的数据，支持核外计算（out-of-core computing）.</li>
    <li>自动处理缺失值.</li>
    <li>支持增量学习，可以在已有模型的基础上使用新样本进行再次学习.</li>
</ul>

<h2 class="sigil_not_in_toc">XGBoost最简示例</h2>

<p>下面是在python环境中使用XGBoost训练一个简单模型的示例程序</p>

<p>输入是一个csv文件，文件中每一行代表一条记录，前8列的数值是记录所对应的特征，最后一列有两个取值，0和1，代表样本属于的两个分类。</p>

<!-- HTML generated using hilite.me --><div style="background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">import</span> loadtxt
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">xgboost</span> <span style="color: #006699; font-weight: bold">import</span> XGBClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.model_selection</span> <span style="color: #006699; font-weight: bold">import</span> train_test_split
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> accuracy_score

dataset <span style="color: #555555">=</span> loadtxt(<span style="color: #CC3300">'pima-indians-diabetes.csv'</span>, delimiter<span style="color: #555555">=</span><span style="color: #CC3300">','</span>)
<span style="color: #0099FF; font-style: italic"># https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv</span>
<span style="color: #336666">print</span>(dataset)

X <span style="color: #555555">=</span> dataset[:,<span style="color: #FF6600">0</span>:<span style="color: #FF6600">8</span>]
Y <span style="color: #555555">=</span> dataset[:,<span style="color: #FF6600">8</span>]
seed <span style="color: #555555">=</span> <span style="color: #FF6600">7</span>
test_size <span style="color: #555555">=</span> <span style="color: #FF6600">0.33</span>
X_train, X_test, y_train, y_test <span style="color: #555555">=</span> train_test_split(X, Y, test_size<span style="color: #555555">=</span>test_size, random_state<span style="color: #555555">=</span>seed)

model <span style="color: #555555">=</span> XGBClassifier()
model<span style="color: #555555">.</span>fit(X<span style="color: #555555">=</span>X_train, y<span style="color: #555555">=</span>y_train)
predictions <span style="color: #555555">=</span> model<span style="color: #555555">.</span>predict(X_test)
<span style="color: #336666">print</span>(predictions)

accuracy <span style="color: #555555">=</span> accuracy_score(y_test, predictions)
<span style="color: #336666">print</span>(accuracy<span style="color: #555555">*</span><span style="color: #FF6600">100.0</span>)
</pre>
</div>

<p>输出如下：</p>

<!-- HTML generated using hilite.me --><div style="background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">[[  6.    148.     72.    ...   0.627  50.      1.   ]
 [  1.     85.     66.    ...   0.351  31.      0.   ]
 [  8.    183.     64.    ...   0.672  32.      1.   ]
 ...
 [  5.    121.     72.    ...   0.245  30.      0.   ]
 [  1.    126.     60.    ...   0.349  47.      1.   ]
 [  1.     93.     70.    ...   0.315  23.      0.   ]]
[0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.
 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.
 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1.]
77.95275590551181
</pre>
</div>


<p>可以看到，这个示例中，模型预测的准确度为77.9%</p>
<hr/>
<p class="footer">This blog is written in  EPUB.
The original file is available for <a href="https://github.com/XDcsy/XDcsy.github.io/raw/main/blog.epub">download</a>.</p>
<p class="footer">Feel free to comment by <a href="https://github.com/XDcsy/XDcsy.github.io/issues/new">raising an issue <img alt="GitHub-Mark-32px" src="../Images/GitHub-Mark-32px.png" class="github"/></a>.</p>
</body>
</html>